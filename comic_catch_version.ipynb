{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于显示爬取进度的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# https://blog.csdn.net/u013832707/article/details/73608504\n",
    "import sys\n",
    "\n",
    "class ShowProcess():\n",
    "    \"\"\"\n",
    "    显示处理进度的类\n",
    "    调用该类相关函数即可实现处理进度的显示\n",
    "    \"\"\"\n",
    "    i = 0 # 当前的处理进度\n",
    "    max_steps = 0 # 总共需要处理的次数\n",
    "    max_arrow = 50 #进度条的长度\n",
    "    infoDone = 'done'\n",
    "\n",
    "    # 初始化函数，需要知道总共的处理次数\n",
    "    def __init__(self, max_steps, infoDone = 'Done'):\n",
    "        self.max_steps = max_steps\n",
    "        self.i = 0\n",
    "        self.infoDone = infoDone\n",
    "\n",
    "    # 显示函数，根据当前的处理进度i显示进度\n",
    "    # 效果为[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100.00%\n",
    "    def show_process(self, i=None):\n",
    "        if i is not None:\n",
    "            self.i = i\n",
    "        else:\n",
    "            self.i += 1\n",
    "        num_arrow = int(self.i * self.max_arrow / self.max_steps) #计算显示多少个'>'\n",
    "        num_line = self.max_arrow - num_arrow #计算显示多少个'-'\n",
    "        percent = self.i * 100.0 / self.max_steps #计算完成进度，格式为xx.xx%\n",
    "        process_bar = '[' + '>' * num_arrow + '-' * num_line + ']'\\\n",
    "                      + '%.2f' % percent + '%' + '\\r' #带输出的字符串，'\\r'表示不换行回到最左边\n",
    "        sys.stdout.write(process_bar) #这两句打印字符到终端\n",
    "        sys.stdout.flush()\n",
    "        if self.i >= self.max_steps:\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        print('')\n",
    "        print(self.infoDone)\n",
    "        self.i = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取漫画图片的函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为避免相关法律问题，我就不放出该漫画网站的实际地址了。且由于网址可能会变动，需要根据实际需要进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comic_catch(url):\n",
    "    ####################################\n",
    "    ####################################\n",
    "    print(url)\n",
    "    # Make a request to the webpage\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    ####################################\n",
    "    ####################################\n",
    "    # 获取漫画名\n",
    "    comic_name=str(soup.find_all('title')).split('>')[1].split(' -')[0]\n",
    "    print(comic_name)\n",
    "    ####################################\n",
    "    ####################################\n",
    "    # 建立路径 存储下载的漫画\n",
    "    if not os.path.exists('{}'.format(comic_name)):\n",
    "        os.makedirs('{}'.format(comic_name))\n",
    "    ####################################\n",
    "    ####################################\n",
    "    # 获取漫画页数\n",
    "    comic_page=str(soup.find_all('label')[1]).split('：')[1].split('P')[0]\n",
    "    comic_page=int(comic_page)\n",
    "    #print(comic_page)\n",
    "    ####################################\n",
    "    ####################################\n",
    "    # 获取漫画起始页地址\n",
    "    liTags=soup.find_all('div', attrs={'class': \"pic_box tb\"})[0]\n",
    "    liTags=str(liTags).split('a href=\"')[1].split('\">')[0]\n",
    "    url_pic='漫画网站主页地址'+liTags\n",
    "    #print(url_pic)\n",
    "    url_befor_download=url_pic\n",
    "    ####################################\n",
    "    ####################################\n",
    "    # # 下载漫画\n",
    "    process_bar = ShowProcess(comic_page, 'Processing Finished') # 进度条\n",
    "    for i in range(comic_page):\n",
    "        # 下载并获取下一页地址\n",
    "        # Make a request to the webpage\n",
    "        response = requests.get(url_befor_download)\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all image tags in the HTML\n",
    "        img_tags = soup.find_all('img')\n",
    "        # Find all image tags in the HTML\n",
    "        img_tags = soup.find_all('img')\n",
    "        # 文件名-页码\n",
    "        page_name = (img_tags[2])['alt']\n",
    "        #print(page_name)\n",
    "        if not os.path.exists(comic_name+'/' + page_name+'.jpg'):\n",
    "            # 文件内容-图片下载地址\n",
    "            download_page = (img_tags[2])['src']\n",
    "            download_page='https:'+download_page\n",
    "            #print(download_page)\n",
    "            img_data = requests.get(download_page).content\n",
    "            with open(comic_name+'/' + page_name+'.jpg', 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "        url_befor_download=soup.find_all('link')\n",
    "        url_befor_download=url_befor_download[4]['href']    \n",
    "        url_befor_download='漫画网站主页地址'+url_befor_download \n",
    "        process_bar.show_process() # 进度条\n",
    "    # print('sleep 10s')\n",
    "    # time.sleep(10)\n",
    "    # print('wake up')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次性爬取多本漫画"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提前将 漫画详情页链接地址 存放在 comic_catch_list.txt 文件中，一行存储一个链接地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('comic_catch_list.txt','r') \n",
    "# 读取所有漫画链接\n",
    "comic_list=[]\n",
    "for i in f:\n",
    "    comic_list.append(i.strip('\\n'))\n",
    "f.close()\n",
    "# 开始下载\n",
    "for i in comic_list:\n",
    "    comic_catch(i)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有时会遇到下面的报错"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSLError: HTTPSConnectionPool(host='漫画网站主页地址', port=443): Max retries exceeded with url: //photos-view-id-17204800.html (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种报错可能是网络状况不佳导致的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只爬取一本漫画"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能会遇到无法作为文件夹名字的漫画名，此时需要用合适的文件夹名，手动建立存储漫画的文件夹，建议采用下方对单本漫画的下载方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "####################################\n",
    "url='漫画详情页链接地址'\n",
    "# Make a request to the webpage\n",
    "response = requests.get(url)\n",
    "# Parse the HTML content of the webpage\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "####################################\n",
    "####################################\n",
    "# # 获取漫画名\n",
    "# comic_name=str(soup.find_all('title')).split('>')[1].split(' -')[0]\n",
    "# print(comic_name)\n",
    "# ####################################\n",
    "# ####################################\n",
    "# # 建立路径 存储下载的漫画\n",
    "# if not os.path.exists('{}'.format(comic_name)):\n",
    "#     os.makedirs('{}'.format(comic_name))\n",
    "####################################\n",
    "####################################\n",
    "# 获取漫画页数\n",
    "comic_page=str(soup.find_all('label')[1]).split('：')[1].split('P')[0]\n",
    "comic_page=int(comic_page)\n",
    "#print(comic_page)\n",
    "####################################\n",
    "####################################\n",
    "# 获取漫画起始页地址\n",
    "liTags=soup.find_all('div', attrs={'class': \"pic_box tb\"})[0]\n",
    "liTags=str(liTags).split('a href=\"')[1].split('\">')[0]\n",
    "url_pic='漫画网站主页地址'+liTags\n",
    "#print(url_pic)\n",
    "url_befor_download=url_pic\n",
    "####################################\n",
    "####################################\n",
    "# # 下载漫画\n",
    "process_bar = ShowProcess(comic_page, 'Processing Finished') # 进度条\n",
    "for i in range(comic_page):\n",
    "    # 下载并获取下一页地址\n",
    "    # Make a request to the webpage\n",
    "    response = requests.get(url_befor_download)\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find all image tags in the HTML\n",
    "    img_tags = soup.find_all('img')\n",
    "    # Find all image tags in the HTML\n",
    "    img_tags = soup.find_all('img')\n",
    "    # 文件名-页码\n",
    "    page_name = (img_tags[2])['alt']\n",
    "    #print(page_name)\n",
    "    if not os.path.exists('漫画名'+'/' + page_name+'.jpg'):\n",
    "        # 文件内容-图片下载地址\n",
    "        download_page = (img_tags[2])['src']\n",
    "        download_page='https:'+download_page\n",
    "        #print(download_page)\n",
    "        img_data = requests.get(download_page).content\n",
    "        with open('漫画名'+'/' + page_name+'.jpg', 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "    url_befor_download=soup.find_all('link')\n",
    "    url_befor_download=url_befor_download[4]['href']    \n",
    "    url_befor_download='漫画网站主页地址'+url_befor_download\n",
    "    process_bar.show_process() # 进度条\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
